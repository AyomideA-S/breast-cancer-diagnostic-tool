{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "dcf4b4ec",
      "metadata": {
        "id": "dcf4b4ec"
      },
      "source": [
        "# Model Evaluation Notebook\n",
        "\n",
        "This notebook focuses **only on model performance evaluation**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "314cbe5c",
      "metadata": {
        "id": "314cbe5c"
      },
      "source": [
        "## 1. Load Required Libraries\n",
        "\n",
        "We import the libraries needed for:\n",
        "- data handling\n",
        "- model evaluation metrics\n",
        "- visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "20fd6c76",
      "metadata": {
        "id": "20fd6c76"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        ")\n",
        "\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2b2e4b60",
      "metadata": {
        "id": "2b2e4b60"
      },
      "source": [
        "## 2. Load the Dataset\n",
        "\n",
        "We load the CSV file that was previously cleaned and used for model training.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f7d8720",
      "metadata": {
        "id": "3f7d8720"
      },
      "outputs": [],
      "source": [
        "# Load dataset\n",
        "data = pd.read_csv('cleaned_wbc_data') # This can be adapted according to the right file location.\n",
        "\n",
        "# Display first rows to understand structure\n",
        "data.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "956c5f06",
      "metadata": {
        "id": "956c5f06"
      },
      "source": [
        "## 3. Define True Labels and Predictions\n",
        "\n",
        "- `y_true` comes from the dataset\n",
        "- `y_pred` comes from the trained model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de479824",
      "metadata": {
        "id": "de479824"
      },
      "outputs": [],
      "source": [
        "# Example column names (adjust if necessary)\n",
        "y_true = y_test #\"y_test\" must be defined from the trained model part\n",
        "y_pred = model.predict(X_test) #\"X_test\" must be defined from the trained model part as well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ff547ada",
      "metadata": {
        "id": "ff547ada"
      },
      "source": [
        "## 4. Basic Evaluation Metrics\n",
        "\n",
        "These metrics answer key questions:\n",
        "- **Accuracy**: overall correctness\n",
        "- **Precision**: how reliable positive predictions are\n",
        "- **Recall**: how well positives are detected\n",
        "- **F1-score**: balance between precision and recall\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "93d943ce",
      "metadata": {
        "id": "93d943ce"
      },
      "outputs": [],
      "source": [
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "precision = precision_score(y_true, y_pred)\n",
        "recall = recall_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Precision:\", precision)\n",
        "print(\"Recall:\", recall)\n",
        "print(\"F1-score:\", f1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8997d8c8",
      "metadata": {
        "id": "8997d8c8"
      },
      "source": [
        "## 5. Classification Report\n",
        "\n",
        "This report summarizes all evaluation metrics in one table.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "853447b8",
      "metadata": {
        "id": "853447b8"
      },
      "outputs": [],
      "source": [
        "print(classification_report(y_true, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e4d67b6",
      "metadata": {
        "id": "3e4d67b6"
      },
      "source": [
        "## 6. Confusion Matrix\n",
        "\n",
        "The confusion matrix helps visualize:\n",
        "- True Positives\n",
        "- False Positives\n",
        "- True Negatives\n",
        "- False Negatives\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c9c26e11",
      "metadata": {
        "id": "c9c26e11"
      },
      "outputs": [],
      "source": [
        "cm = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "plt.figure()\n",
        "plt.imshow(cm)\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.colorbar()\n",
        "plt.xlabel(\"Predicted Label/diagnosis\")\n",
        "plt.ylabel(\"True Label/diagnosis\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}